# microtron

Inspired by [picotron](https://github.com/huggingface/picotron) and [nanoGPT](https://github.com/karpathy/nanoGPT), and while taking the course [Scratch to Scale: Large-Scale Training in the Modern World](https://maven.com/walk-with-code/scratch-to-scale), my plan is to build a clean, minimal, and education-oriented 4D parallel implementation (Data Parallel, Tensor Parallel, Pipeline Parallel, and Context Parallel) for pretraining a simple dense model like GPT-2 from scratch.
